# Ручная установка HA-кластера k3s с MySQL как БД и HAProxy как балансирощиками

## Вступление

За основу было взято [видео](https://www.youtube.com/watch?v=APsZJbnluXg) от ютубера Techno Tim. В том, что касается непосредственно k3s, данный текст полностью повторяет руководство Тима. Однако в видео ни MySQL, ни используемый балансировщик никак не резервируется, что не очень-то и соответствует определению High Availability. 

Также отмечу, что руководство Тима было создано в те времена, когда k3s не позволял использовать etcd в качестве базы для HA, и именно поэтому в нем рассказывается про вариант с MySQL. Сейчас было бы правильней именно etcd на серверах k3s, что позволит избавиться от как минимум четырех лишних VM.

Начинал писать данный текст как шпаргалку для себя, но в процессе решил облагородить и сделать репозиторий публичным.

## Как устроен кластер

![](https://docs.k3s.io/img/k3s-architecture-ha-external.svg)Изображение из официального [руководства](https://docs.k3s.io/architecture) для общего понимания. У меня схема реализована несколько иначе.

1) Сервера k3s, они же Server Nodes. Управляют агентами. Для HA нужно как минимум два.
2) Агенты k3s, они же Agent Nodes. Рабочая нагрузка приходится непосредственно на них. В нашем случае их может быть от одного до бесконечности.
3) Балансировщики серверов. Здесь у меня первое отличие и от официальной схемы, и от варианта Тима. В моём случае балансирощиков от двух штук, они резервируют друг друга, а через них к серверу подключается не только пользователь, но и ноды.
4) База данных, а конкретно MySQL. И у Тима, и на схеме БД всего одна. В моём случае их две.
5) Балансировщики БД. Поскольку HA подразумевает под собой отсутствие ручного переключения БД на резервную в случае падения основной, подключаться к ней нужно через балансировщик. Также балансировщик нужно дублировать, поэтому у меня их два.

## На чем всё это разворачиваем

Для развёртывания я использовал свой сервер Proxmox. Балансировщики запускал в LXC-контейнерах с Ubuntu 20.04, всё остальное в виртуальных машинах с Ubuntu 20.04. Ресурсов выделял минимум: по одному ядру и по гигабайту оперативной памяти. Swap и IPv6 везде отключены, все работы проводил от имени пользователя root. Также у меня настроен DNS-сервер, а все хосты общаются между собой по доменным именам. Повторять это не обязательно, ничто не мешает использовать в конфигах IP.

## Подготовка балансировщиков

Поскольку все участники кластера общаются между собой через балансировщики, начнем с них. Создадим 4 сервера:

 - k3s-balancer-01
 - k3s-balancer-02
 - k3s-db-balancer-01
 - k3s-db-balancer-02

На каждом из них нам потребуется по два приложения: keepalived и HAProxy.

    apt install haproxy keepalived -y

Затем идем в файлы этого репозитория, и складываем соответствующие конфиги в соответствующие директории соответствующих серверов: haproxy.cfg в /etc/haproxy (перезаписываем оригинальный файл и при желании удаляем всё остальное), keepalived.conf в /etc/keepalived.

Теперь нужно либо внести записи "k3s-server-01.lab.lan", "k3s-server-02.lab.lan", "k3s-db-01.lab.lan" и "k3s-db-02.lab.lan" в DNS-сервер, либо поменять данные адреса в конфигах на IP, которые будем использовать на данных серверах. У HAProxy есть неприятная особенность: если он не может зарезолвить адрес, указанный в конфиге, то не запустится.

Также в пункте virtual_ipaddress в конфигах keepalived указаны виртуальные IP, по одному на каждую пару балансировщиков. Их также нужно поменять на свой свой адреса и, в случае использования DNS, назначить A-записи для k3s-balancer.lab.lan и k3s-db-balancer.lab.lan.

После того, как это сделано, запускаем данные приложения.

    systemctl enable --now keepalived
    systemctl enable --now haproxy

Поскольку в Ubuntu 20.04 брандмауэр изначально выключен, на этом с балансировщиками всё.    

  ## Конфигурация балансировщиков
  
  ### keepalived
  
keepalived - приложение, которое позволяет назначить серверу плавающий IP-адрес, который перейдет на другой сервер в случае недоступности основного. Или же, как это сделано в данном конфиге, при недоступности приложения.

У всех четырех запущенных keepalived конфигурация почти не отличается, поэтому для примера разберем только один из них.

```
vrrp_script check_haproxy_stats {
        script "/usr/bin/curl 127.0.0.1:9000/stats"
        interval 2
}

vrrp_instance k3s-balancer {
        state MASTER
        interface eth0
        virtual_router_id 3
        priority 100
        advert_int 1
        authentication {
              auth_type PASS
              auth_pass k3s
        }
        virtual_ipaddress {
              192.168.11.241/24
        }
        track_script {
              check_haproxy_stats
        }
}
```

Первый блок, vrrp_script, описывает проверку доступности HAProxy, а конкретно его страницы со статистикой. Каждые две секунды работает curl, если он получает ответ, то проверка проходит, если нет, значит keepalived считает, что HAProxy лежит. Проверка простая как палка, а еще сама по себе ни на что не влияет, влияение включается ниже по конфигу.

Блок vrrp_instance - это непосредственно то, что описывает переключение адреса.

 - state MASTER или BACKUP - изначальное состояние при запуске приложения, которое затем перекрывается параметром priority.
 - interface eth0 - здесь нужно указать, какой именно сетевой интерфейс необходимо использовать и для назначения плавающего адреса, и для отслеживания состояния второго keepalived. Интерфейсы одной группы keepalived при данной конфигурации должны быть в одном L2-сегменте (а еще точнее должны пропускать multicast-трафик). Ничего страшного, если на данном интерфейсе уже висит родной адрес.
 - virtual_router_id 3 - ID должен быть у каждой **пары** балансировщиков свой совпадающий.
- priority 100 - сервер с большим приоритетом получает статус MASTER.
- advert_int 1 - проверяем доступность второго балансировщика раз в секунду.
- authentication - пока готовил этот текст, обнаружил, что сами разработчики не рекомендуют использовать данный параметр. Менять работающий конфиг не буду, но отметку сделаю.
- virtual_ipaddress - собственно, плавающий адрес, к которому будут обращаться члены кластера.
- track_script - указывает, что при переключении адресов нужно смотреть не только на доступность второго балансировщика, но и на результаты проверки, которая была описана в первом блоке конфигурации.

  ### HAProxy балансировщика серверов

HAProxy в нашем случае используется в качестве L4-прокси, т.е. прокси, пропускающего через себя весь трафик, не заглядывая внутрь. В нашем случае это нужно для балансировки, т.е. динамического распределения трафика по серверам, до которых он должен дойти.

Т.к. у балансирощиков k3s-серверов и баз данных несколько разные настройки, разберем их по отдельности. Начнем с балансировщика серверов:

```
listen stats
  bind 127.0.0.1:9000
  mode http
  stats enable
  stats hide-version
  stats refresh 10s
  stats show-node
  stats uri /stats

frontend k3s
  bind *:6443
  use_backend k3s

backend k3s
  mode tcp
  option ssl-hello-chk
  server k3s-server-01 k3s-server-01.lab.lan:6443 check 
  server k3s-server-02 k3s-server-02.lab.lan:6443 check backup
```

Первый блок, listen с названием stats, включает страницу HAProxy с технической информацией. Именно доступность этой страницы проверяет keepalived, а еще она незаменима при дебаге и проверки конфигурации.

- bind 127.0.0.1:9000 - поменяйте на "*:9000", если хотите открыть данную страницу у себя в браузере. Порт может быть любым, разве что у меня данная страница не заработала на 80-м.
- mode http и stats enable - необходимы для технической страницы.
- stats hide-version - скрывает версию HAProxy с технической страницы, чтобы у злоумышленников было меньше информации. Не нужно, если держать страницу доступной только локалхосту.
- stats refresh 10s - делает так, чтобы открытая страница обновлялась каждые 10 секунд. Не нужно, если держать страницу доступной только локалхосту.
- stats show-node - показывает на странице имя сервера, на котором запущен HAProxy. Незаменимо при конфигурации keepalived, не нужно, если держать страницу доступной только локалхосту.
- stats uri /stats - путь, по которому страница доступна. В данном случае она находится по пути 127.0.0.1:9000/stats.

Второй блок, frontend, описывает то, что касается входящих подключений. В нашем случае это:

- k3s - название фронтенда.
- bind *:6443 - слушать порт 6443 на всех адресах.
- use_backend k3s - для данного фронтенда использовать бэкенд с названием k3s. Наверное, стоило дать фронту и бэку разные названия, но теперь уже не буду менять рабочий конфиг.

Третий блок, backend, описывает, что делать с входящим трафиком:

- k3s - название бэкенда.
- mode tcp - использовать режим L4-прокси, т.е. перенаправлять весь трафик, не заглядывая внутрь.
- option ssl-hello-chk - при использовании check (см. ниже) проверять доступность SSL на стороне сервера-получателя. k3s использует для работы https на порту 6443. HAProxy при данной конфигурации не проверяет доступность всего хоста и не смотрит, что находится за этим портом. На нем есть https - сервер работает. Нет - помечаем сервер как упавший.
- server k3s-server-01 k3s-server-01.lan.lan:6443 check - первый из двух наших серверов приложений. Сначала идёт название в рамках HAProxy - k3s-server-01. Оно может быть любым. Затем адрес сервера и используемый порт. Напомню, что здесь вам либо нужно вписать свой IP-адрес, либо иметь соответствующую запись в DNS. check - говорим HAProxy проверять, доступен ли данный сервер или нет.
- server k3s-server-02 k3s-server-02.lab.lan:6443 check backup - всё то же самое, но второй сервер мы отметили как резервный. Соединения с ним не будут устанавливаться, пока активен основной.

Отмечу, что в моём конфиге не указаны таймауты для соединений. Из-за этого HAProxy будет ругаться при запуске, но всё будет работать. Таймаут по умолчанию - две секунды. Т.е. при падении первого сервера переключение на второй произойдет через две секунды.

  ### HAProxy балансировщика баз данных

Первые два блока (техническая страница и фронтенд) у балансировщиков БД такие же, как у балансировщиков серверов. Однако в третьем блоке есть небольшие, но очень важные отличия.

```
backend k3s-db
  mode tcp
  option mysql-check user haproxy-check
  server k3s-db-01 k3s-db-01.lab.lan:3306 check on-marked-up shutdown-backup-sessions rise 30 
  server k3s-db-02 k3s-db-02.lab.lan:3306 check backup rise 30
```

- option mysql-check user haproxy-check - вместо проверки наличия SSL мы проверяем доступность MySQL. Конкретно HAProxy пытается подключиться к серверу БД и если видит любой ответ, считает, что сервер жив. Однако сервер MySQL не любит, когда к нему активно пытаются подключиться без успеха, и банит такие подключения по IP. Поэтому мы указываем для проверки пользователя haproxy-check, создание которого описано в разделе, посвященном MySQL-серверу.
- server k3s-db-01 k3s-db-01.lab.lan:3306 check on-marked-up shutdown-backup-sessions rise 30 - первые два новых пункта обозначают, что когда данный основной сервер поднимается, балансировщик обрывает соединения со всеми серверами, отмеченными как backup. Это очень важно, т.к. сервера MySQL у нас работают в режиме master-master, и именно эта настройка значительно уменьшит вероятность сплит-брейна. Последняя новая опция, rise 30, говорит отмечать сервер работающим только после 30 успешных проверок (т.е. одной минуты), чтобы дать время поднявшемуся серверу синхронизировать изменения.

Важно отметить, что такая настройка балансировщика не даёт гаранированной защиты от расхождения баз при использовании репликации master-master. Вполне возможна ситуация, когда в момент переключения базы одна из них отстаёт от другой, или при обрыве соединения транзакция не будет завершена. При использовании больших баз не стоит использовать данное автоматическое переключение туда-обратно. Однако в ситуации домашнего k3s, когда используется мелкая база, я считаю использование подобной схемы оправданным.
